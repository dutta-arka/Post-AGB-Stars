{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35f798a6-4e09-42ea-a45b-7275a473f2a2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Star Selection\n",
    "In the post_AGB.csv file, we see three categories distinguished as 1, 2, and 3. We choose 1 (most likely post-AGB stars) and 2 (post-AGB candidate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "86c192b4-f5d7-48fd-8131-fd2a6aad98a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[565507868441719424, 4715635535640762240, 532078488709487360, 459182413984008448, 513671461473684352, 433515788197481984, 255225480926107392, 3238918336374596864, 3422437684728294528, 2968265509022275840, 4758015524139610880, 2902505745786910080, 3334854780347915520, 3336558507975208448, 994259335315643520, 3105987960396950784, 3159640386918214528, 3108327343185135872, 5617989266685365120, 3156171118495247360, 3032030620730261376, 5620444471847839232, 3151417586128916864, 5597822402371118336, 5545800762036628736, 5520238967817034880, 5707613169577769600, 5540178478053582592, 5277809440015969792, 5521628033275348480, 5462428643590805248, 5351069693654349952, 5241806275407841664, 5237007177683569536, 5335866849446446080, 5335709477519159936, 5343168568718268800, 5335675087769798272, 3589047952995134720, 5335102207846402176, 3920735495441657728, 3469106382752903168, 6130448958959242240, 6060828565581083264, 6073662099660289536, 3497154104039422848, 6084869868362934144, 6066902993687172608, 5857811238294426752, 5869845594859880064, 3624198034063995008, 6083719439934104832, 6083708479176016128, 6070128028770373888, 6063703586653222144, 5783415017323438848, 5870113880062711552, 5865398796206273152, 5864661779824561664, 5896479309853592448, 5849962851220246016, 5849958457496943744, 5878583349370477312, 5880708980232408448, 5893945588395282304, 5903310335089068416, 5886573569080505216, 1194929381434604800, 1369896865785991424, 5996617434442714880, 5933063252888145920, 5935061172876722176, 5932016212933920384, 5831295999979910656, 4351018375858237952, 1324742534573959424, 5943047784868946688, 1328057763997734144, 4334241408966611328, 5969973999973524224, 4365451214021224320, 5963059480546004608, 4568163710366782848, 4128590918794710272, 5972460442434579968, 4108072721831278976, 4136944866387751552, 5972489407656030720, 4109553493474085504, 5960186761599871488, 4162959693758887424, 5975083327400970752, 4110550475656804864, 5946845601071213696, 4117592465329067008, 5955201232284272384, 4053492968991830400, 4124125282361429504, 1367102319545324288, 4120637086125583360, 4120632688077368192, 5954670408684703872, 4067343654354938112, 4582795323914832000, 4172337943816530432, 4042544062195042176, 4035907203854415488, 4579182637944779264, 4580154606223711872, 4065347387968755328, 4046476534251259904, 4155847571502019840, 4093689116883710592, 4104509518289438720, 2096072103492979584, 6736747708089687936, 4099619470274753408, 4508727788268978176, 4072427555640528000, 4266422531037747200, 4285847607265349632, 4203848980711226112, 4282499452616310912, 6715619076008049792, 4293369057089082112, 4264026012336768000, 4520072476226779520, 6435349718091211264, 4515084168071571840, 2049984454412871296, 4318134628803970816, 1825500124644105728, 2032364166432389760, 4301238979044890496, 4240112390324832384, 2049034819957965312, 2031794791233840128, 2033763428091006720, 2020571869841643392, 2026709201998745472, 6871175064823382912, 6879196723703009920, 2034134414507432064, 2030200671149815424, 4190636669164572928, 2055174630333744384, 2060806470651334912, 1803364717856260736, 1836195688380634368, 2054521833963867008, 2056435602670418688, 1869422453048750336, 2193902559325301760, 2197298400984984064, 1731164844433296128, 2168803045330976768, 2179471159976791168, 6831062200578042624, 6616993471402951680, 6824212243136821248, 1976077657917533952, 2005246464463628800, 1958757291756223104, 2006425553228658816, 2594762641717531008, 6385794694664872320, 1932229409071269248, 2015785313459952128]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the file paths\n",
    "file_path = \"post_AGB.csv\"  # Replace with your actual file path\n",
    "\n",
    "# Load the CSV files\n",
    "df = pd.read_csv(file_path, delimiter=',')\n",
    "\n",
    "# Extract Gaia source IDs from the other CSV file (assuming it's the first column)\n",
    "ids_to_keep = set(df.iloc[:, 0].astype(str))\n",
    "\n",
    "# Filter rows in df based on conditions\n",
    "filtered_df = df[(df['Vickers category'] <= 2) & (df['parallax'] > 0.01)]\n",
    "\n",
    "# Keep only those rows where the Gaia source ID is in ids_to_keep and the last column is not empty\n",
    "filtered_df = filtered_df[filtered_df.iloc[:, -1].notna() & filtered_df.iloc[:, 0].astype(str).isin(ids_to_keep)]\n",
    "\n",
    "# Extract the first column (Gaia source ID)\n",
    "first_column_list = filtered_df.iloc[:, 0].tolist()\n",
    "\n",
    "# Print the list of first column values\n",
    "print(first_column_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b10c0db2-cd03-46dd-8b71-9487459f1ed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "168"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaiadr3_ids = first_column_list\n",
    "len(gaiadr3_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f86e66-0e1d-4148-baff-92f122e4ee2c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a2089a15-410d-4914-bc40-f877e6ecf080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "pyvo version 1.5.2 \n",
      "\n",
      "TAP service APPLAUSE \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pyvo as vo\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Define the parameters\n",
    "name = 'APPLAUSE'\n",
    "url = 'https://www.plate-archive.org/tap'\n",
    "token = 'Token d62cc50a37a9d01149f6de294ee9ab0193207569'\n",
    "\n",
    "# Function to chunk the list into groups of specified size\n",
    "def chunk_list(data_list, chunk_size):\n",
    "    for i in range(0, len(data_list), chunk_size):\n",
    "        yield data_list[i:i + chunk_size]\n",
    "\n",
    "# Function to process each chunk\n",
    "def process_chunk(chunk):\n",
    "    # Format the gaiaedr3_id list for the SQL query\n",
    "    ids_str = ', '.join(f\"'{id}'\" for id in chunk)\n",
    "    \n",
    "    # Create the query string\n",
    "    qstr = f\"\"\"\n",
    "    SELECT plate_id, scan_id, source_id, solution_num, gaiaedr3_id\n",
    "    FROM applause_dr4.source_xmatch \n",
    "    WHERE gaiaedr3_id IN ({ids_str})\n",
    "    \"\"\"\n",
    "    \n",
    "    # Setup the TAP service session\n",
    "    tap_session = requests.Session()\n",
    "    tap_session.headers['Authorization'] = token\n",
    "    tap_service = vo.dal.TAPService(url, session=tap_session)\n",
    "    \n",
    "    # Submit the query\n",
    "    lang = 'PostgreSQL'\n",
    "    job = tap_service.submit_job(qstr, language=lang, QUEUE=\"1h\")\n",
    "    job.run()\n",
    "    \n",
    "    # Wait for job completion\n",
    "    job.wait(phases=[\"COMPLETED\", \"ERROR\", \"ABORTED\"], timeout=600.)\n",
    "    \n",
    "    # Raise an error if the job failed\n",
    "    job.raise_if_error()\n",
    "    \n",
    "    # Fetch results\n",
    "    return job.fetch_result()\n",
    "\n",
    "# Print pyvo version and TAP service name\n",
    "print('\\npyvo version %s \\n' % vo.__version__)\n",
    "print('TAP service %s \\n' % name)\n",
    "\n",
    "# Initialize the result dictionary\n",
    "gaiaedr3_to_plates = {}\n",
    "\n",
    "# Function to process chunks with retries\n",
    "def process_chunks_with_retries(gaiadr3_ids, chunk_size, max_retries=3):\n",
    "    for chunk in chunk_list(gaiadr3_ids, chunk_size):\n",
    "        retries = 0\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                results = process_chunk(chunk)\n",
    "                # Process results into a dictionary\n",
    "                for row in results:\n",
    "                    gaiaedr3_id = str(row['gaiaedr3_id'])  # Convert to string\n",
    "                    plate_info = {\n",
    "                        'plate_id': row['plate_id'],\n",
    "                        'source_id': row['source_id'],\n",
    "                        'scan_id': row['scan_id'],\n",
    "                        'solution_num': row['solution_num']\n",
    "                    }\n",
    "                    if gaiaedr3_id not in gaiaedr3_to_plates:\n",
    "                        gaiaedr3_to_plates[gaiaedr3_id] = []\n",
    "                    gaiaedr3_to_plates[gaiaedr3_id].append(plate_info)\n",
    "                break  # Exit the retry loop if processing is successful\n",
    "            except Exception as e:\n",
    "                retries += 1\n",
    "                print(f\"Error processing chunk {chunk} (retry {retries}/{max_retries}): {e}\")\n",
    "                if retries == max_retries:\n",
    "                    print(f\"Failed to process chunk after {max_retries} retries. Skipping to next chunk.\")\n",
    "\n",
    "# Process all chunks with retries\n",
    "process_chunks_with_retries(gaiadr3_ids, chunk_size=150)\n",
    "\n",
    "# Print the resulting dictionary\n",
    "# print(gaiaedr3_to_plates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3025c22c-8ad0-4b5d-8bdc-ea0327a51875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary saved to gaiaedr3_to_plates.txt\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define a custom encoder class\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return super(NumpyEncoder, self).default(obj)\n",
    "\n",
    "# Assuming 'gaiaedr3_to_plates_selected' is your dictionary\n",
    "# Write the dictionary to a .txt file in JSON format with NumPyEncoder\n",
    "# with open('pAGB_gaiaedr3_to_plates_selected.txt', 'w') as file:\n",
    "with open('pAGB_gaiaedr3_to_plates.txt', 'w') as file:\n",
    "    json.dump(gaiaedr3_to_plates, file, indent=4, ensure_ascii=False, cls=NumpyEncoder)\n",
    "\n",
    "print('Dictionary saved to gaiaedr3_to_plates.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2bc00e85-841a-4548-9fa3-7d2f622ab17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8745833333333333 hrs\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Read the gaiaedr3_to_plates.txt file to obtain source_ids\n",
    "# with open('pAGB_gaiaedr3_to_plates_selected.txt', 'r') as file:\n",
    "with open('pAGB_gaiaedr3_to_plates.txt', 'r') as file:\n",
    "    gaiaedr3_to_plates = json.load(file)\n",
    "\n",
    "# Extract source_ids from gaiaedr3_to_plates dictionary\n",
    "source_ids = [entry['source_id'] for entries in gaiaedr3_to_plates.values() for entry in entries]\n",
    "\n",
    "print(len(source_ids) * 25 / (100 * 3600), \"hrs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "82c33c12-420b-4c7d-8511-fa1c356e5365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "import json\n",
    "import pyvo as vo\n",
    "\n",
    "# Define the parameters\n",
    "url = 'https://www.plate-archive.org/tap'\n",
    "token = 'Token d62cc50a37a9d01149f6de294ee9ab0193207569'\n",
    "\n",
    "# Function to chunk the list into groups of specified size\n",
    "def chunk_list(data_list, chunk_size):\n",
    "    for i in range(0, len(data_list), chunk_size):\n",
    "        yield data_list[i:i + chunk_size]\n",
    "\n",
    "# Read the gaiaedr3_to_plates.txt file to obtain source_ids\n",
    "# with open('pAGB_gaiaedr3_to_plates_selected.txt', 'r') as file:\n",
    "with open('pAGB_gaiaedr3_to_plates.txt', 'r') as file:\n",
    "    gaiaedr3_to_plates = json.load(file)\n",
    "\n",
    "# Extract source_ids from gaiaedr3_to_plates dictionary\n",
    "source_ids = [entry['source_id'] for entries in gaiaedr3_to_plates.values() for entry in entries]\n",
    "\n",
    "# Initialize dictionary to store calibration information for each source_id\n",
    "source_calib_info = {}\n",
    "\n",
    "# Setup a retry strategy\n",
    "retry_strategy = Retry(\n",
    "    total=3,\n",
    "    backoff_factor=1,\n",
    "    status_forcelist=[429, 500, 502, 503, 504]\n",
    ")\n",
    "adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "session = requests.Session()\n",
    "session.mount(\"https://\", adapter)\n",
    "session.headers['Authorization'] = token\n",
    "\n",
    "# Initialize TAP service\n",
    "tap_service = vo.dal.TAPService(url, session=session)\n",
    "lang = 'PostgreSQL'\n",
    "\n",
    "# Function to process each chunk for source calibration information\n",
    "def process_source_calib_chunk(chunk):\n",
    "    # Format the source_id list for the SQL query\n",
    "    ids_str = ', '.join(f\"'{id}'\" for id in chunk)\n",
    "    \n",
    "    # Create the query string\n",
    "    qstr = f\"\"\"\n",
    "    SELECT bpmag, bpmag_error, rpmag, rpmag_error, natmag, natmag_error, natmag_plate, natmag_correction, natmag_residual, source_id, gaiaedr3_id, airmass, zenith_angle, ra_icrs, dec_icrs, phot_calib_flags, gaiaedr3_gmag, gaiaedr3_bp_rp, gaiaedr3_dist, cat_natmag\n",
    "    FROM applause_dr4.source_calib \n",
    "    WHERE source_id IN ({ids_str}) AND phot_calib_flags = 0\n",
    "    \"\"\"\n",
    "    \n",
    "    # Submit the query\n",
    "    job = tap_service.submit_job(qstr, language=lang, QUEUE=\"1h\")\n",
    "    job.run()\n",
    "    \n",
    "    # Wait for job completion\n",
    "    job.wait(phases=[\"COMPLETED\", \"ERROR\", \"ABORTED\"], timeout=600.)\n",
    "    \n",
    "    # Raise an error if the job failed\n",
    "    job.raise_if_error()\n",
    "    \n",
    "    # Fetch results\n",
    "    return job.fetch_result()\n",
    "\n",
    "# Function to process chunks with retries\n",
    "def process_chunks_with_retries(source_ids, chunk_size, max_retries=3):\n",
    "    for chunk in chunk_list(source_ids, chunk_size):\n",
    "        retries = 0\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                results = process_source_calib_chunk(chunk)\n",
    "                # Process results into a dictionary\n",
    "                for row in results:\n",
    "                    source_id = str(row['source_id'])\n",
    "                    calib_info = {\n",
    "                        'gaiaedr3_id': row['gaiaedr3_id'],\n",
    "                        'bpmag': row['bpmag'],\n",
    "                        'bpmag_error': row['bpmag_error'],\n",
    "                        'rpmag': row['rpmag'],\n",
    "                        'rpmag_error': row['rpmag_error'],\n",
    "                        'natmag': row['natmag'],\n",
    "                        'natmag_error': row['natmag_error'],\n",
    "                        'natmag_plate': row['natmag_plate'],\n",
    "                        'natmag_correction': row['natmag_correction'],\n",
    "                        'natmag_residual': row['natmag_residual'],\n",
    "                        'cat_natmag': row['cat_natmag'],\n",
    "                        'airmass': row['airmass'],\n",
    "                        'zenith_angle': row['zenith_angle'],\n",
    "                        'ra_icrs': row['ra_icrs'],\n",
    "                        'dec_icrs': row['dec_icrs'],\n",
    "                        'phot_calib_flags': row['phot_calib_flags'],\n",
    "                        'gaiaedr3_gmag': row['gaiaedr3_gmag'],\n",
    "                        'gaiaedr3_bp_rp': row['gaiaedr3_bp_rp'],\n",
    "                        'gaiaedr3_dist': row['gaiaedr3_dist']\n",
    "                    }\n",
    "                    source_calib_info[source_id] = calib_info\n",
    "                break  # Exit the retry loop if processing is successful\n",
    "            except Exception as e:\n",
    "                retries += 1\n",
    "                print(f\"Error processing chunk {chunk} (retry {retries}/{max_retries}): {e}\")\n",
    "                if retries == max_retries:\n",
    "                    print(f\"Failed to process chunk after {max_retries} retries. Skipping to next chunk.\")\n",
    "\n",
    "# Process all chunks with retries\n",
    "process_chunks_with_retries(source_ids, chunk_size=200)\n",
    "\n",
    "# Print the resulting dictionary\n",
    "# print(source_calib_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "442f5214-c0a6-4370-bb74-c1821819981b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source calibration information saved to source_calib_info_selected.txt\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define a custom encoder class\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return super(NumpyEncoder, self).default(obj)\n",
    "\n",
    "# Save the source calibration information to a .txt file in JSON format\n",
    "# with open('pAGB_source_calib_info_selected.txt', 'w') as file:\n",
    "with open('pAGB_source_calib_info.txt', 'w') as file:\n",
    "    json.dump(source_calib_info, file, indent=4, ensure_ascii=False, cls=NumpyEncoder)\n",
    "\n",
    "print('Source calibration information saved to source_calib_info_selected.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "565b029f-3523-40fb-ab22-42ff92c06283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique plates: 17987\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pyvo as vo\n",
    "import json\n",
    "\n",
    "# Define the parameters\n",
    "name = 'APPLAUSE'\n",
    "url = 'https://www.plate-archive.org/tap'\n",
    "token = 'Token d62cc50a37a9d01149f6de294ee9ab0193207569'\n",
    "\n",
    "# Function to chunk the list into groups of specified size\n",
    "def chunk_list(data_list, chunk_size):\n",
    "    for i in range(0, len(data_list), chunk_size):\n",
    "        yield data_list[i:i + chunk_size]\n",
    "\n",
    "# Function to process the plate details query\n",
    "def process_plate_details_query(plate_ids):\n",
    "    # Format the plate_id list for the SQL query\n",
    "    ids_str = ', '.join(f\"'{id}'\" for id in plate_ids)\n",
    "    \n",
    "    # Create the query string\n",
    "    qstr = f\"\"\"\n",
    "    SELECT plate_id, plate_num, plate_quality, date_orig, observatory, air_temperature\n",
    "    FROM applause_dr4.plate\n",
    "    WHERE plate_id IN ({ids_str})\n",
    "    \"\"\"\n",
    "    \n",
    "    # Setup the TAP service session\n",
    "    tap_session = requests.Session()\n",
    "    tap_session.headers['Authorization'] = token\n",
    "    tap_service = vo.dal.TAPService(url, session=tap_session)\n",
    "    \n",
    "    # Submit the query\n",
    "    lang = 'PostgreSQL'\n",
    "    job = tap_service.submit_job(qstr, language=lang, QUEUE=\"1h\")\n",
    "    job.run()\n",
    "    \n",
    "    # Wait for job completion\n",
    "    job.wait(phases=[\"COMPLETED\", \"ERROR\", \"ABORTED\"], timeout=600.)\n",
    "    \n",
    "    # Raise an error if the job failed\n",
    "    job.raise_if_error()\n",
    "    \n",
    "    # Fetch results\n",
    "    return job.fetch_result()\n",
    "\n",
    "# Read the gaiaedr3_to_plates.txt file\n",
    "# with open('pAGB_gaiaedr3_to_plates_selected.txt', 'r') as file:\n",
    "with open('pAGB_gaiaedr3_to_plates.txt', 'r') as file:\n",
    "    gaiaedr3_to_plates = json.load(file)\n",
    "\n",
    "# Extract all unique plate_ids\n",
    "plate_ids = set()\n",
    "for plate_list in gaiaedr3_to_plates.values():\n",
    "    for plate_info in plate_list:\n",
    "        plate_ids.add(plate_info['plate_id'])\n",
    "\n",
    "# Notify the total number of unique plates\n",
    "total_plates = len(plate_ids)\n",
    "print(f'Total number of unique plates: {total_plates}')\n",
    "\n",
    "# Initialize list to hold all plate details\n",
    "all_plate_details = []\n",
    "\n",
    "# Process each chunk of plate_ids\n",
    "for chunk in chunk_list(list(plate_ids), 150):\n",
    "    results = process_plate_details_query(chunk)\n",
    "    \n",
    "    # Convert results to a list of dictionaries and add to the main list\n",
    "    for row in results:\n",
    "        all_plate_details.append({\n",
    "            'plate_id': row['plate_id'],\n",
    "            'plate_num': row['plate_num'],\n",
    "            'plate_quality': row['plate_quality'],\n",
    "            'date_orig': row['date_orig'],\n",
    "            'observatory': row['observatory'],\n",
    "            'air_temperature': row['air_temperature']\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "93ffc26f-aa7d-48c5-b336-4e27b1450adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique plate details saved to unique_plate_details.txt\n"
     ]
    }
   ],
   "source": [
    "# Save the unique plate details to a new .txt file in JSON format\n",
    "# with open('pAGB_unique_plate_details_selected.txt', 'w') as file:\n",
    "with open('pAGB_unique_plate_details.txt', 'w') as file:\n",
    "    json.dump(all_plate_details, file, indent=4, ensure_ascii=False, cls=NumpyEncoder)\n",
    "\n",
    "print('Unique plate details saved to unique_plate_details.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d61cfa9d-fc60-46e5-8d71-3e2ad9c861df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pyvo\n",
    "\n",
    "service_url = 'https://www.plate-archive.org/tap'\n",
    "\n",
    "tap_session = requests.Session()\n",
    "tap_session.headers['Authorization'] = 'Token d62cc50a37a9d01149f6de294ee9ab0193207569'\n",
    "\n",
    "tap_service = pyvo.dal.TAPService(service_url, session=tap_session)\n",
    "completed_jobs = tap_service.get_job_list(phases='COMPLETED')\n",
    "for job in completed_jobs:\n",
    "     job = pyvo.dal.AsyncTAPJob(service_url + '/async/' + job.jobid,\n",
    "session=tap_session)\n",
    "     job.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e1e225-75f0-4150-a5f9-b72354aeae55",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Crossmatching\n",
    "Please download the combined_refined.csv file from drive to run the code below or run Data_processing.ipynb beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4601ac86-b756-478f-8327-510616c4b880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Read the .csv file and store the data in a list\n",
    "# csv_file_path = 'HS.csv'\n",
    "csv_file_path = 'combined_refined.csv'\n",
    "csv_data = []\n",
    "\n",
    "with open(csv_file_path, mode='r') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    for row in csv_reader:\n",
    "        csv_data.append(row)\n",
    "\n",
    "# Read the .txt file and load the JSON data\n",
    "# txt_file_path = 'pAGB_gaiaedr3_to_plates_selected.txt'\n",
    "txt_file_path = 'pAGB_gaiaedr3_to_plates.txt'\n",
    "\n",
    "with open(txt_file_path, mode='r') as txt_file:\n",
    "    gaia_data = json.load(txt_file)\n",
    "\n",
    "# Function to find matching plate_ids under the same GAIA ID\n",
    "def find_matching_plate_ids(plate_id_1, plate_id_2):\n",
    "    for gaia_id, plates in gaia_data.items():\n",
    "        plates_dict = {plate['plate_id']: plate for plate in plates}\n",
    "        if plate_id_1 in plates_dict and plate_id_2 in plates_dict:\n",
    "            return gaia_id, plates_dict[plate_id_1], plates_dict[plate_id_2]\n",
    "    return None, None, None\n",
    "\n",
    "# Function to calculate the days gap between two dates\n",
    "def calculate_days_gap(date1, date2):\n",
    "    date_format = \"%Y-%m-%d\"\n",
    "    d1 = datetime.strptime(date1, date_format)\n",
    "    d2 = datetime.strptime(date2, date_format)\n",
    "    return abs((d2 - d1).days)\n",
    "\n",
    "# Process the csv data to find consecutive pairs and match them\n",
    "output_data = {}\n",
    "\n",
    "for i in range(0, len(csv_data) - 1, 2):\n",
    "    plate_id_1 = int(csv_data[i]['plate_id'])\n",
    "    plate_id_2 = int(csv_data[i+1]['plate_id'])\n",
    "    date_orig_1 = csv_data[i]['date_orig']\n",
    "    date_orig_2 = csv_data[i+1]['date_orig']\n",
    "    emulsion_1 = csv_data[i]['emulsion']\n",
    "    emulsion_2 = csv_data[i+1]['emulsion']\n",
    "    \n",
    "    # Check if emulsion_1 and emulsion_2 are not the same\n",
    "    if emulsion_1 != emulsion_2:\n",
    "        # Find matching plate IDs in the GAIA data\n",
    "        gaia_id, plate_1, plate_2 = find_matching_plate_ids(plate_id_1, plate_id_2)\n",
    "        \n",
    "        if plate_1 and plate_2:\n",
    "            source_id_1 = plate_1['source_id']\n",
    "            source_id_2 = plate_2['source_id']\n",
    "            days_gap = calculate_days_gap(date_orig_1, date_orig_2)\n",
    "            \n",
    "            entry = {\n",
    "                \"source_id_1\": source_id_1,\n",
    "                \"plate_id_1\": plate_id_1,\n",
    "                \"date_orig_1\": date_orig_1,\n",
    "                \"emulsion_1\": emulsion_1,\n",
    "                \"source_id_2\": source_id_2,\n",
    "                \"plate_id_2\": plate_id_2,\n",
    "                \"date_orig_2\": date_orig_2,\n",
    "                \"emulsion_2\": emulsion_2,\n",
    "                \"days_gap\": days_gap\n",
    "            }\n",
    "            \n",
    "            if gaia_id not in output_data:\n",
    "                output_data[gaia_id] = []\n",
    "            output_data[gaia_id].append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7e2015f3-d012-4732-909c-a36d894857e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete. Output written to results2.json\n"
     ]
    }
   ],
   "source": [
    "# Write the output data to a JSON file\n",
    "output_file_path = 'results2.json'\n",
    "\n",
    "with open(output_file_path, mode='w') as output_file:\n",
    "    json.dump(output_data, output_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(\"Processing complete. Output written to\", output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0d01c58e-41db-4e97-89c8-4affa2842793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated data saved to updated_results2.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "import pyvo as vo\n",
    "import pandas as pd\n",
    "\n",
    "# Load the JSON file containing GAIA IDs and corresponding source and plate IDs\n",
    "# with open('results1.json', 'r') as file:\n",
    "with open('results2.json', 'r') as file:\n",
    "    gaia_data = json.load(file)\n",
    "\n",
    "# Extract all unique source IDs from the JSON data\n",
    "source_ids = set()\n",
    "for entries in gaia_data.values():\n",
    "    for entry in entries:\n",
    "        source_ids.add(int(entry['source_id_1']))\n",
    "        source_ids.add(int(entry['source_id_2']))\n",
    "\n",
    "# Convert the set to a list for easier processing\n",
    "source_ids = list(source_ids)\n",
    "\n",
    "# Function to chunk the list into groups of specified size\n",
    "def chunk_list(data_list, chunk_size):\n",
    "    for i in range(0, len(data_list), chunk_size):\n",
    "        yield data_list[i:i + chunk_size]\n",
    "\n",
    "# Function to query the database for color terms in batches\n",
    "def query_color_terms_batch(source_ids):\n",
    "    # Convert source_ids list to string for SQL query\n",
    "    source_ids_str = ', '.join([f\"'{source_id}'\" for source_id in source_ids])\n",
    "    \n",
    "    # Create the query string\n",
    "    query = f\"\"\"\n",
    "    SELECT source_id, plate_id, color_term, natmag, natmag_error, gaiaedr3_gmag, gaiaedr3_bp_rp\n",
    "    FROM applause_dr4.source_calib\n",
    "    WHERE source_id IN ({source_ids_str}) AND phot_calib_flags = 0\n",
    "    \"\"\"\n",
    "    \n",
    "    # Setup the TAP service session\n",
    "    tap_session = requests.Session()\n",
    "    tap_session.headers['Authorization'] = 'Token d62cc50a37a9d01149f6de294ee9ab0193207569'\n",
    "    tap_service = vo.dal.TAPService('https://www.plate-archive.org/tap', session=tap_session)\n",
    "    \n",
    "    # Submit the query\n",
    "    lang = 'PostgreSQL'\n",
    "    job = tap_service.submit_job(query, language=lang, QUEUE=\"1h\")\n",
    "    job.run()\n",
    "    \n",
    "    # Wait for job completion\n",
    "    job.wait(phases=[\"COMPLETED\", \"ERROR\", \"ABORTED\"], timeout=600.)\n",
    "    \n",
    "    # Raise an error if the job failed\n",
    "    job.raise_if_error()\n",
    "    \n",
    "    # Fetch results\n",
    "    result = job.fetch_result()\n",
    "    \n",
    "    # Convert to pandas DataFrame\n",
    "    df_result = result.to_table().to_pandas()\n",
    "    \n",
    "    return df_result\n",
    "\n",
    "# Initialize dictionaries to store color terms, errors, and Gaia data\n",
    "color_term_map = {}\n",
    "color_error_map = {}\n",
    "gmag_map = {}\n",
    "bp_rp_map = {}\n",
    "\n",
    "# Query color terms and Gaia data for each chunk of source IDs and store in dictionaries\n",
    "for chunk in chunk_list(source_ids, 150):\n",
    "    df_results = query_color_terms_batch(chunk)\n",
    "    \n",
    "    # Process each result and store in dictionaries\n",
    "    for index, row in df_results.iterrows():\n",
    "        source_id = row['source_id']\n",
    "        color_term = row['color_term']\n",
    "        color_term_error = row['natmag_error']\n",
    "        gmag = row['gaiaedr3_gmag']\n",
    "        bp_rp = row['gaiaedr3_bp_rp']\n",
    "        \n",
    "        color_term_map[source_id] = color_term\n",
    "        color_error_map[source_id] = color_term_error\n",
    "        gmag_map[source_id] = gmag\n",
    "        bp_rp_map[source_id] = bp_rp\n",
    "\n",
    "# Update the original JSON data with the new source information\n",
    "for gaia_id, entries in gaia_data.items():\n",
    "    for entry in entries:\n",
    "        source_id_1 = entry['source_id_1']\n",
    "        source_id_2 = entry['source_id_2']\n",
    "        \n",
    "        # Add color term, color term error, gmag, and bp_rp for source_id_1\n",
    "        if source_id_1 in color_term_map:\n",
    "            entry['color_term_1'] = color_term_map[source_id_1]\n",
    "            entry['color_term_error_1'] = color_error_map[source_id_1]\n",
    "            entry['gmag_1'] = gmag_map[source_id_1]\n",
    "            entry['bp_rp_1'] = bp_rp_map[source_id_1]\n",
    "        \n",
    "        # Add color term, color term error, gmag, and bp_rp for source_id_2\n",
    "        if source_id_2 in color_term_map:\n",
    "            entry['color_term_2'] = color_term_map[source_id_2]\n",
    "            entry['color_term_error_2'] = color_error_map[source_id_2]\n",
    "            entry['gmag_2'] = gmag_map[source_id_2]\n",
    "            entry['bp_rp_2'] = bp_rp_map[source_id_2]\n",
    "\n",
    "# Save the updated JSON data to a file\n",
    "output_filename = 'updated_results2.json'\n",
    "with open(output_filename, 'w') as file:\n",
    "    json.dump(gaia_data, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f'Updated data saved to {output_filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7b0e037d-1781-4a00-b06c-3270b1d256cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved to cleaned_results2.json\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the updated JSON file\n",
    "with open(output_filename, 'r') as file:\n",
    "    updated_gaia_data = json.load(file)\n",
    "\n",
    "# Function to check if an entry has any NaN values for color terms\n",
    "def has_nan_values(entry):\n",
    "    return (\n",
    "        pd.isna(entry.get('color_term_1')) or\n",
    "        pd.isna(entry.get('color_term_error_1')) or\n",
    "        pd.isna(entry.get('color_term_2')) or\n",
    "        pd.isna(entry.get('color_term_error_2'))\n",
    "    )\n",
    "\n",
    "# Clean up the updated data\n",
    "cleaned_gaia_data = {}\n",
    "\n",
    "for gaia_id, entries in updated_gaia_data.items():\n",
    "    cleaned_entries = []\n",
    "    for entry in entries:\n",
    "        if not has_nan_values(entry):\n",
    "            cleaned_entries.append(entry)\n",
    "    if cleaned_entries:\n",
    "        cleaned_gaia_data[gaia_id] = cleaned_entries\n",
    "\n",
    "# Save the cleaned JSON data to a file\n",
    "# cleaned_output_filename = 'cleaned_results1.json'\n",
    "cleaned_output_filename = 'cleaned_results2.json'\n",
    "with open(cleaned_output_filename, 'w') as file:\n",
    "    json.dump(cleaned_gaia_data, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f'Cleaned data saved to {cleaned_output_filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "17348d09-f7f5-485b-9939-a1b4f49c5e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pyvo\n",
    "\n",
    "service_url = 'https://www.plate-archive.org/tap'\n",
    "\n",
    "tap_session = requests.Session()\n",
    "tap_session.headers['Authorization'] = 'Token d62cc50a37a9d01149f6de294ee9ab0193207569'\n",
    "\n",
    "tap_service = pyvo.dal.TAPService(service_url, session=tap_session)\n",
    "completed_jobs = tap_service.get_job_list(phases='COMPLETED')\n",
    "for job in completed_jobs:\n",
    "     job = pyvo.dal.AsyncTAPJob(service_url + '/async/' + job.jobid,\n",
    "session=tap_session)\n",
    "     job.delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
